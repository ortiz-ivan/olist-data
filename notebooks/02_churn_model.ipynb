{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16db978b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías cargadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, f1_score, classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:.3f}')\n",
    "\n",
    "print(\"Librerías cargadas correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0a6cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customers cargado: 99441 filas, 5 columnas\n",
      "geolocation cargado: 1000163 filas, 5 columnas\n",
      "items cargado: 112650 filas, 7 columnas\n",
      "payments cargado: 103886 filas, 5 columnas\n",
      "reviews cargado: 99224 filas, 7 columnas\n",
      " Contiene valores nulos: 145903\n",
      "orders cargado: 99441 filas, 8 columnas\n",
      " Contiene valores nulos: 4908\n",
      "products cargado: 32951 filas, 9 columnas\n",
      " Contiene valores nulos: 2448\n",
      "sellers cargado: 3095 filas, 4 columnas\n",
      "categories cargado: 71 filas, 2 columnas\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cargar_datasets(ruta_base: str) -> dict:\n",
    "    archivos = {\n",
    "        'customers': 'olist_customers_dataset.csv',\n",
    "        'geolocation': 'olist_geolocation_dataset.csv',\n",
    "        'items': 'olist_order_items_dataset.csv',\n",
    "        'payments': 'olist_order_payments_dataset.csv',\n",
    "        'reviews': 'olist_order_reviews_dataset.csv',\n",
    "        'orders': 'olist_orders_dataset.csv',\n",
    "        'products': 'olist_products_dataset.csv',\n",
    "        'sellers': 'olist_sellers_dataset.csv',\n",
    "        'categories': 'product_category_name_translation.csv'\n",
    "    }\n",
    "    \n",
    "    data = {}\n",
    "    for key, archivo in archivos.items():\n",
    "        try:\n",
    "            df = pd.read_csv(f\"{ruta_base}/{archivo}\")\n",
    "            data[key] = df\n",
    "            print(f\"{key} cargado: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "            if df.isnull().sum().sum() > 0:\n",
    "                print(f\" Contiene valores nulos: {df.isnull().sum().sum()}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Archivo no encontrado: {archivo}\")\n",
    "        \n",
    "    return data\n",
    "\n",
    "# Llamada a la función\n",
    "data = cargar_datasets(\"../data/raw\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebfe1028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets base preparados correctamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "orders = data['orders'][['customer_id', 'order_id', 'order_purchase_timestamp']].copy()\n",
    "reviews = data['reviews'][['review_id', 'order_id', 'review_creation_date']].copy()\n",
    "\n",
    "# Convertir fechas a tipo datetime\n",
    "orders['order_purchase_timestamp'] = pd.to_datetime(orders['order_purchase_timestamp'])\n",
    "reviews['review_creation_date'] = pd.to_datetime(reviews['review_creation_date'])\n",
    "\n",
    "print(\"Datasets base preparados correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de0cf717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados por cliente a partir de órdenes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "total_orders\n",
       "1    99441\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Número total de pedidos por cliente\n",
    "df_orders = (\n",
    "    orders.groupby('customer_id')\n",
    "    .agg(\n",
    "        total_orders=('order_id', 'count'),\n",
    "        last_purchase=('order_purchase_timestamp', 'max'),\n",
    "        first_purchase=('order_purchase_timestamp', 'min')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calcular promedio de días entre compras (frecuencia)\n",
    "df_orders['avg_days_between_orders'] = (\n",
    "    (df_orders['last_purchase'] - df_orders['first_purchase']).dt.days /\n",
    "    (df_orders['total_orders'] - 1)\n",
    ").replace([np.inf, np.nan], 0)\n",
    "\n",
    "print(\"Datos agregados por cliente a partir de órdenes.\")\n",
    "df_orders.head()\n",
    "df_orders['total_orders'].value_counts().sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c04f8970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features generadas: 11 columnas\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_unique_id</th>\n",
       "      <th>n_pedidos</th>\n",
       "      <th>avg_payment_value</th>\n",
       "      <th>total_payment</th>\n",
       "      <th>total_price</th>\n",
       "      <th>avg_freight</th>\n",
       "      <th>n_productos_distintos</th>\n",
       "      <th>n_categorias_distintas</th>\n",
       "      <th>avg_review_score</th>\n",
       "      <th>avg_dias_entre_pedidos</th>\n",
       "      <th>std_dias_entre_pedidos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000366f3b9a7992bf8c76cfdf3221e2</td>\n",
       "      <td>1</td>\n",
       "      <td>141.900</td>\n",
       "      <td>141.900</td>\n",
       "      <td>129.900</td>\n",
       "      <td>12.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000b849f77a49e4a4ce2b2a4ca5be3f</td>\n",
       "      <td>1</td>\n",
       "      <td>27.190</td>\n",
       "      <td>27.190</td>\n",
       "      <td>18.900</td>\n",
       "      <td>8.290</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000f46a3911fa3c0805444483337064</td>\n",
       "      <td>1</td>\n",
       "      <td>86.220</td>\n",
       "      <td>86.220</td>\n",
       "      <td>69.000</td>\n",
       "      <td>17.220</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>3.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000f6ccb0745a6a4b88665a16c9f078</td>\n",
       "      <td>1</td>\n",
       "      <td>43.620</td>\n",
       "      <td>43.620</td>\n",
       "      <td>25.990</td>\n",
       "      <td>17.630</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004aac84e0df4da2b147fca70cf8255</td>\n",
       "      <td>1</td>\n",
       "      <td>196.890</td>\n",
       "      <td>196.890</td>\n",
       "      <td>180.000</td>\n",
       "      <td>16.890</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>5.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 customer_unique_id  n_pedidos  avg_payment_value  \\\n",
       "0  0000366f3b9a7992bf8c76cfdf3221e2          1            141.900   \n",
       "1  0000b849f77a49e4a4ce2b2a4ca5be3f          1             27.190   \n",
       "2  0000f46a3911fa3c0805444483337064          1             86.220   \n",
       "3  0000f6ccb0745a6a4b88665a16c9f078          1             43.620   \n",
       "4  0004aac84e0df4da2b147fca70cf8255          1            196.890   \n",
       "\n",
       "   total_payment  total_price  avg_freight  n_productos_distintos  \\\n",
       "0        141.900      129.900       12.000                  1.000   \n",
       "1         27.190       18.900        8.290                  1.000   \n",
       "2         86.220       69.000       17.220                  1.000   \n",
       "3         43.620       25.990       17.630                  1.000   \n",
       "4        196.890      180.000       16.890                  1.000   \n",
       "\n",
       "   n_categorias_distintas  avg_review_score  avg_dias_entre_pedidos  \\\n",
       "0                   1.000             5.000                     NaN   \n",
       "1                   1.000             4.000                     NaN   \n",
       "2                   1.000             3.000                     NaN   \n",
       "3                   1.000             4.000                     NaN   \n",
       "4                   1.000             5.000                     NaN   \n",
       "\n",
       "   std_dias_entre_pedidos  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def generar_features_clientes(df_orders, df_items, df_products):\n",
    "    \"\"\"\n",
    "    Genera features agregadas por cliente sin usar recency (días desde la última compra):\n",
    "    - Frecuencia y gasto\n",
    "    - Diversidad de productos y categorías\n",
    "    - Promedio de reseñas\n",
    "    - Tiempo promedio y desviación entre pedidos\n",
    "    \"\"\"\n",
    "    # Frecuencia de compra y monetario\n",
    "    freq_monetary = df_orders.groupby('customer_unique_id').agg({\n",
    "        'order_id':'nunique',\n",
    "        'payment_value':['mean','sum'],\n",
    "        'total_price':'sum',\n",
    "        'total_freight':'mean'\n",
    "    })\n",
    "    freq_monetary.columns = ['n_pedidos','avg_payment_value','total_payment','total_price','avg_freight']\n",
    "    freq_monetary.reset_index(inplace=True)\n",
    "\n",
    "    # Diversidad de productos y categorías\n",
    "    # Merge items con info de producto para obtener categoría\n",
    "    df_items_full = df_items.merge(\n",
    "        df_products[['product_id','product_category_name']],\n",
    "        on='product_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Merge con customer_id vía orders\n",
    "    df_items_unique = df_items_full.merge(\n",
    "        df_orders[['order_id','customer_unique_id']],\n",
    "        on='order_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    diversidad = df_items_unique.groupby('customer_unique_id').agg({\n",
    "        'product_id':'nunique',\n",
    "        'product_category_name':'nunique'\n",
    "    }).rename(columns={\n",
    "        'product_id':'n_productos_distintos',\n",
    "        'product_category_name':'n_categorias_distintas'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Reviews: promedio de puntuación\n",
    "    reviews = df_orders.groupby('customer_unique_id').agg({\n",
    "        'review_score':'mean'\n",
    "    }).rename(columns={'review_score':'avg_review_score'}).reset_index()\n",
    "\n",
    "    # Tiempo entre pedidos histórico\n",
    "    df_sorted = df_orders.sort_values(['customer_unique_id','order_purchase_timestamp'])\n",
    "    df_sorted['prev_order_date'] = df_sorted.groupby('customer_unique_id')['order_purchase_timestamp'].shift(1)\n",
    "    df_sorted['dias_entre_pedidos'] = (df_sorted['order_purchase_timestamp'] - df_sorted['prev_order_date']).dt.days\n",
    "\n",
    "    tiempo_entre = df_sorted.groupby('customer_unique_id')['dias_entre_pedidos'].agg(['mean','std']).reset_index()\n",
    "    tiempo_entre.rename(columns={'mean':'avg_dias_entre_pedidos','std':'std_dias_entre_pedidos'}, inplace=True)\n",
    "\n",
    "    # Merge de todas las features en un solo DataFrame\n",
    "    features = freq_monetary.merge(diversidad, on='customer_unique_id', how='left')\n",
    "    features = features.merge(reviews, on='customer_unique_id', how='left')\n",
    "    features = features.merge(tiempo_entre, on='customer_unique_id', how='left')\n",
    "\n",
    "    print(f\"Features generadas: {features.shape[1]} columnas\")\n",
    "    return features\n",
    "\n",
    "features_clientes = generar_features_clientes(df, data['items'], data['products'])\n",
    "features_clientes.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6ee99092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (23361, 11), Test: (5841, 11)\n",
      "Distribución de clases en entrenamiento:\n",
      " churn\n",
      "1   0.978\n",
      "0   0.022\n",
      "Name: proportion, dtype: float64\n",
      "⚡ Clase minoritaria <20%, aplicando SMOTE para balancear...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     31\u001b[39m         \u001b[38;5;28mprint\u001b[39m(pd.Series(y_train).value_counts(normalize=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_test, y_train, y_test\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mpreparar_datos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_clientes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclientes_churn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mpreparar_datos\u001b[39m\u001b[34m(features, labels, test_size, random_state)\u001b[39m\n\u001b[32m     27\u001b[39m X_test = X_test.select_dtypes(include=[\u001b[33m'\u001b[39m\u001b[33mint64\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mfloat64\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mInt64\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mFloat64\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     28\u001b[39m smote = SMOTE(random_state=random_state)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m X_train, y_train = \u001b[43msmote\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDistribución de clases después de SMOTE:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(pd.Series(y_train).value_counts(normalize=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\olist-data\\.venv\\Lib\\site-packages\\imblearn\\base.py:202\u001b[39m, in \u001b[36mBaseSampler.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, **params):\n\u001b[32m    182\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[32m    183\u001b[39m \n\u001b[32m    184\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m \u001b[33;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\olist-data\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\olist-data\\.venv\\Lib\\site-packages\\imblearn\\base.py:99\u001b[39m, in \u001b[36mSamplerMixin.fit_resample\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m     97\u001b[39m check_classification_targets(y)\n\u001b[32m     98\u001b[39m arrays_transformer = ArraysTransformer(X, y)\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m X, y, binarize_y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[38;5;28mself\u001b[39m.sampling_strategy_ = check_sampling_strategy(\n\u001b[32m    102\u001b[39m     \u001b[38;5;28mself\u001b[39m.sampling_strategy, y, \u001b[38;5;28mself\u001b[39m._sampling_type\n\u001b[32m    103\u001b[39m )\n\u001b[32m    105\u001b[39m output = \u001b[38;5;28mself\u001b[39m._fit_resample(X, y, **params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\olist-data\\.venv\\Lib\\site-packages\\imblearn\\base.py:157\u001b[39m, in \u001b[36mBaseSampler._check_X_y\u001b[39m\u001b[34m(self, X, y, accept_sparse)\u001b[39m\n\u001b[32m    155\u001b[39m     accept_sparse = [\u001b[33m\"\u001b[39m\u001b[33mcsr\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcsc\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    156\u001b[39m y, binarize_y = check_target_type(y, indicate_one_vs_all=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\olist-data\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2969\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2970\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2971\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\olist-data\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1368\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1364\u001b[39m     )\n\u001b[32m   1366\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1385\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1387\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\olist-data\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\olist-data\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\olist-data\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nSMOTE does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "def preparar_datos(features: pd.DataFrame, labels: pd.DataFrame, test_size: float = 0.2, random_state: int = 42):\n",
    "\n",
    "    # Merge features y etiquetas\n",
    "    df_modelo = features.merge(labels, on='customer_unique_id', how='inner')\n",
    "    df_modelo.dropna(subset=['churn'], inplace=True)\n",
    "\n",
    "    # Separar X e y\n",
    "    X = df_modelo.drop(columns=['customer_unique_id','churn'])\n",
    "    y = df_modelo['churn']\n",
    "\n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, stratify=y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    # Ver distribución de clases en entrenamiento\n",
    "    prop_train = y_train.value_counts(normalize=True)\n",
    "    print(\"Distribución de clases en entrenamiento:\\n\", prop_train)\n",
    "\n",
    "    # Aplicar SMOTE si la clase minoritaria < 20%\n",
    "    if prop_train.min() < 0.2:\n",
    "        print(\"⚡ Clase minoritaria <20%, aplicando SMOTE para balancear...\")\n",
    "        \n",
    "        X_train = X_train.select_dtypes(include=['int64','float64','Int64','Float64'])\n",
    "        X_test = X_test.select_dtypes(include=['int64','float64','Int64','Float64'])\n",
    "        smote = SMOTE(random_state=random_state)\n",
    "        X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "        print(\"Distribución de clases después de SMOTE:\")\n",
    "        print(pd.Series(y_train).value_counts(normalize=True))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = preparar_datos(features_clientes, clientes_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdf63dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear imputador que reemplaza NaN por la media de la columna\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Ajustar e imputar en X_train y X_test\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "097df226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas datetime que se eliminarán: []\n"
     ]
    }
   ],
   "source": [
    "# Detectar columnas datetime\n",
    "datetime_cols = X_train.select_dtypes(include=['datetime64[ns]']).columns\n",
    "print(\"Columnas datetime que se eliminarán:\", list(datetime_cols))\n",
    "\n",
    "# Eliminar columnas datetime de X_train y X_test\n",
    "X_train = X_train.drop(columns=datetime_cols)\n",
    "X_test = X_test.drop(columns=datetime_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e7f7c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\olist-data\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression: ROC-AUC=1.000, F1=1.000\n",
      "RandomForest: ROC-AUC=1.000, F1=1.000\n",
      "GradientBoosting: ROC-AUC=0.585, F1=1.000\n",
      "KNN: ROC-AUC=0.833, F1=1.000\n",
      "NaiveBayes: ROC-AUC=0.998, F1=0.997\n",
      "SVM: ROC-AUC=1.000, F1=0.998\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>0.998</td>\n",
       "      <td>0.997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.833</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.585</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               modelo  roc_auc    f1\n",
       "1        RandomForest    1.000 1.000\n",
       "0  LogisticRegression    1.000 1.000\n",
       "5                 SVM    1.000 0.998\n",
       "4          NaiveBayes    0.998 0.997\n",
       "3                 KNN    0.833 1.000\n",
       "2    GradientBoosting    0.585 1.000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def entrenar_y_evaluar_modelos(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    modelos = {\n",
    "        \"LogisticRegression\": LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "        \"RandomForest\": RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42),\n",
    "        \"GradientBoosting\": HistGradientBoostingClassifier(random_state=42),\n",
    "        \"KNN\": KNeighborsClassifier(),\n",
    "        \"NaiveBayes\": GaussianNB(),\n",
    "        \"SVM\": SVC(probability=True, class_weight='balanced')\n",
    "    }\n",
    "\n",
    "    resultados = []\n",
    "    for nombre, modelo in modelos.items():\n",
    "        modelo.fit(X_train, y_train)\n",
    "        y_pred = modelo.predict(X_test)\n",
    "        y_prob = modelo.predict_proba(X_test)[:,1] if hasattr(modelo, \"predict_proba\") else None\n",
    "        \n",
    "        roc = roc_auc_score(y_test, y_prob) if y_prob is not None else np.nan\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        resultados.append({'modelo': nombre, 'roc_auc': roc, 'f1': f1})\n",
    "        print(f\"{nombre}: ROC-AUC={roc:.3f}, F1={f1:.3f}\")\n",
    "\n",
    "    return pd.DataFrame(resultados).sort_values(by='roc_auc', ascending=False)\n",
    "\n",
    "resultados_modelos = entrenar_y_evaluar_modelos(X_train, X_test, y_train, y_test)\n",
    "resultados_modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4778f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def graficar_metricas(y_test, y_prob, titulo=\"Curva ROC y Precision-Recall\"):\n",
    "    \"\"\"\n",
    "    Genera las curvas ROC y Precision-Recall para evaluar desempeño del modelo.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "    auc_pr = auc(recall, precision)\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(fpr, tpr, label=f'ROC AUC={auc_roc:.2f}')\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.title(\"Curva ROC\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(recall, precision, label=f'PR AUC={auc_pr:.2f}')\n",
    "    plt.title(\"Curva Precision-Recall\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.suptitle(titulo)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
